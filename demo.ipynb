{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Image Search\n",
    "\n",
    "## Part 1: Loading the eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src import meta_clip\n",
    "from src import eval_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68,154 image files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c5f2f\">\n",
       "  <caption>queries</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c5f2f_level0_col0\" class=\"col_heading level0 col0\" >query_id</th>\n",
       "      <th id=\"T_c5f2f_level0_col1\" class=\"col_heading level0 col1\" >query_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c5f2f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c5f2f_row0_col0\" class=\"data row0 col0\" >10002456_caption_00</td>\n",
       "      <td id=\"T_c5f2f_row0_col1\" class=\"data row0 col1\" >Several men in hard hats are operating a giant pulley system.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5f2f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c5f2f_row1_col0\" class=\"data row1 col0\" >10002456_caption_01</td>\n",
       "      <td id=\"T_c5f2f_row1_col1\" class=\"data row1 col1\" >Workers look down from up above on a piece of equipment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5f2f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c5f2f_row2_col0\" class=\"data row2 col0\" >10002456_caption_02</td>\n",
       "      <td id=\"T_c5f2f_row2_col1\" class=\"data row2 col1\" >Two men working on a machine wearing hard hats.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5f2f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c5f2f_row3_col0\" class=\"data row3 col0\" >10002456_caption_03</td>\n",
       "      <td id=\"T_c5f2f_row3_col1\" class=\"data row3 col1\" >Four men on top of a tall structure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5f2f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c5f2f_row4_col0\" class=\"data row4 col0\" >10002456_caption_04</td>\n",
       "      <td id=\"T_c5f2f_row4_col1\" class=\"data row4 col1\" >Three men on a large rig.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7bf80f9350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a8877\">\n",
       "  <caption>qrels</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a8877_level0_col0\" class=\"col_heading level0 col0\" >query_id</th>\n",
       "      <th id=\"T_a8877_level0_col1\" class=\"col_heading level0 col1\" >document_id</th>\n",
       "      <th id=\"T_a8877_level0_col2\" class=\"col_heading level0 col2\" >relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a8877_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a8877_row0_col0\" class=\"data row0 col0\" >10002456_caption_00</td>\n",
       "      <td id=\"T_a8877_row0_col1\" class=\"data row0 col1\" >10002456.jpg</td>\n",
       "      <td id=\"T_a8877_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8877_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a8877_row1_col0\" class=\"data row1 col0\" >10002456_caption_01</td>\n",
       "      <td id=\"T_a8877_row1_col1\" class=\"data row1 col1\" >10002456.jpg</td>\n",
       "      <td id=\"T_a8877_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8877_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a8877_row2_col0\" class=\"data row2 col0\" >10002456_caption_02</td>\n",
       "      <td id=\"T_a8877_row2_col1\" class=\"data row2 col1\" >10002456.jpg</td>\n",
       "      <td id=\"T_a8877_row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8877_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a8877_row3_col0\" class=\"data row3 col0\" >10002456_caption_03</td>\n",
       "      <td id=\"T_a8877_row3_col1\" class=\"data row3 col1\" >10002456.jpg</td>\n",
       "      <td id=\"T_a8877_row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8877_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a8877_row4_col0\" class=\"data row4 col0\" >10002456_caption_04</td>\n",
       "      <td id=\"T_a8877_row4_col1\" class=\"data row4 col1\" >10002456.jpg</td>\n",
       "      <td id=\"T_a8877_row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7bd7cc45d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dir = Path(\"vehicle_image_search\")\n",
    "image_dir = dataset_dir / \"images\"\n",
    "filenames = sorted(image_dir.glob(\"*.jpg\"))\n",
    "print(f\"Found {len(filenames):,} image files\")\n",
    "df_queries = pd.read_csv(dataset_dir / \"queries.csv\")\n",
    "display(df_queries.head().style.set_caption(\"queries\"))\n",
    "df_qrels = pd.read_csv(\"vehicle_image_search/qrels.csv\")\n",
    "display(df_qrels.head().style.set_caption(\"qrels\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lqLCZLbhLocB"
   },
   "source": [
    "## Part 2: Embed teh data using the MetaCLIP 1.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up round-robin inference across devices.\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "loaded_clip = meta_clip.load_model()\n",
    "model_on_devices = [deepcopy(loaded_clip.model).to(i) for i in range(torch.cuda.device_count())]\n",
    "\n",
    "def round_robin_image_batches(filenames, batch_size: int):\n",
    "    for i, start in enumerate(range(0, len(filenames), batch_size)):\n",
    "        end = start + batch_size\n",
    "        name_batch = filenames[start:end]\n",
    "        device_i = i % torch.cuda.device_count()\n",
    "        image_batch = meta_clip.read_image_batch(\n",
    "            image_processor_fn=loaded_clip.image_processor_fn,\n",
    "            image_filepaths=name_batch\n",
    "        )\n",
    "        yield image_batch, \"image\", device_i\n",
    "\n",
    "def round_robin_query_batches(queries: list[str], batch_size: int):\n",
    "    for i, start in enumerate(range(0, len(queries), batch_size)):\n",
    "        end = start + batch_size\n",
    "        query_batch = queries[start:end]\n",
    "        device_i = i % torch.cuda.device_count()\n",
    "        batch_tensor = loaded_clip.text_tokenizer_fn(query_batch)\n",
    "        yield batch_tensor, \"text\", device_i\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_on_device(args: tuple[torch.Tensor, Literal[\"image\", \"text\"], int]) -> np.ndarray:\n",
    "    batch, kind, device_i = args\n",
    "    model_on_device = model_on_devices[device_i]\n",
    "    batch_on_device = batch.to(device_i)\n",
    "    assert kind in (\"image\", \"text\")\n",
    "    encode = model_on_device.encode_image if kind == \"image\" else model_on_device.encode_text\n",
    "    embeddings = encode(batch_on_device)\n",
    "    embeddings = F.normalize(embeddings, dim=-1)\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be78b5a7e61f42e88e4d6a5b8f29ee44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding:   0%|          | 0/68154 [00:00<?, ?image/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embed images.\n",
    "batch_iter = round_robin_image_batches(filenames, BATCH_SIZE)\n",
    "res = []\n",
    "with tqdm(total=len(filenames), unit=\"image\", desc=\"embedding\") as pbar, ThreadPool(torch.cuda.device_count()) as pool:\n",
    "    for chunk in pool.imap(embed_on_device, batch_iter):\n",
    "        res.append(chunk)\n",
    "        pbar.update(chunk.shape[0])\n",
    "res = np.row_stack(res)\n",
    "np.save(\"./clip_embedded_images.npy\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cfd9450ef54be089db4c68e30346c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding:   0%|          | 0/10725 [00:00<?, ?query/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embed queries.\n",
    "batch_iter = round_robin_query_batches(df_queries[\"query_text\"].tolist(), BATCH_SIZE)\n",
    "res = []\n",
    "with tqdm(total=len(df_queries), unit=\"query\", desc=\"embedding\") as pbar, ThreadPool(torch.cuda.device_count()) as pool:\n",
    "    for chunk in pool.imap(embed_on_device, batch_iter):\n",
    "        res.append(chunk)\n",
    "        pbar.update(chunk.shape[0])\n",
    "res = np.row_stack(res)\n",
    "np.save(\"./clip_embedded_queries.npy\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2B: Jina CLIP v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/conda/envs/pytorch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/conda/envs/pytorch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/conda/envs/pytorch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/conda/envs/pytorch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/conda/envs/pytorch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/conda/envs/pytorch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet einops timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "jina_model = AutoModel.from_pretrained('jinaai/jina-clip-v2', trust_remote_code=True)\n",
    "jina_on_devices = [deepcopy(jina_model).to(i) for i in range(torch.cuda.device_count())]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_on_device_jina(args: tuple[list[Path] | list[str], Literal[\"image\", \"text\"], int]) -> np.ndarray:\n",
    "    batch, kind, device_i = args\n",
    "    model_on_device = jina_on_devices[device_i]\n",
    "    assert kind in (\"image\", \"text\")\n",
    "    encode = model_on_device.encode_image if kind == \"image\" else model_on_device.encode_text\n",
    "    return encode(batch)\n",
    "\n",
    "\n",
    "# Embed images and texts.\n",
    "image_batches = [\n",
    "    ([str(fp) for fp in filenames[start:start+BATCH_SIZE]], \"image\", i % torch.cuda.device_count())\n",
    "    for i, start in enumerate(range(0, len(filenames), BATCH_SIZE))\n",
    "]\n",
    "query_texts = df_queries[\"query_text\"].tolist()\n",
    "text_batches = [\n",
    "    (query_texts[start:start+BATCH_SIZE], \"text\", i % torch.cuda.device_count())\n",
    "    for i, start in enumerate(range(0, len(query_texts), BATCH_SIZE))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df032e7853447d2a3341e0eb8abffc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding:   0%|          | 0/68154 [00:00<?, ?image/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1507484a7441c890a65f25f5f86d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding:   0%|          | 0/10725 [00:00<?, ?query/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b14f48817a4ac7b5357a77e024f88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e3748bdd1e47d4bc00bf92f9e2a925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcea6f92c124109b48f843b5f1b9bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = []\n",
    "with tqdm(total=len(filenames), unit=\"image\", desc=\"embedding\") as pbar, ThreadPool(torch.cuda.device_count()) as pool:\n",
    "    for chunk in pool.imap(embed_on_device_jina, image_batches):\n",
    "        res.append(chunk)\n",
    "        pbar.update(chunk.shape[0])\n",
    "res = np.row_stack(res)\n",
    "np.save(\"./clip_embedded_images_jina.npy\", res)\n",
    "\n",
    "res = []\n",
    "with tqdm(total=len(query_texts), unit=\"query\", desc=\"embedding\") as pbar, ThreadPool(torch.cuda.device_count()) as pool:\n",
    "    for chunk in pool.imap(embed_on_device_jina, text_batches):\n",
    "        res.append(chunk)\n",
    "        pbar.update(chunk.shape[0])\n",
    "res = np.row_stack(res)\n",
    "np.save(\"./clip_embedded_queries_jina.npy\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_image = np.load(\"./clip_embedded_images.npy\")\n",
    "emb_text = np.load(\"./clip_embedded_queries.npy\")\n",
    "emb_image_jina = np.load(\"./clip_embedded_images_jina.npy\")\n",
    "emb_text_jina = np.load(\"./clip_embedded_queries_jina.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = df_qrels[\"query_id\"].tolist()\n",
    "doc_ids = [filename.name.removeprefix(\"flickr30k_\") for filename in filenames]\n",
    "qrels = {qid: {docid: relevance} for qid, docid, relevance in df_qrels.itertuples(index=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.4 s, sys: 25.8 s, total: 1min 11s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run = eval_retrieval.dense_retrieval_run(\n",
    "    emb_queries=emb_text,\n",
    "    emb_docs=emb_image,\n",
    "    query_ids=query_ids,\n",
    "    doc_ids=doc_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.8 s, sys: 30.2 s, total: 1min 15s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_jina = eval_retrieval.dense_retrieval_run(\n",
    "    emb_queries=emb_text_jina,\n",
    "    emb_docs=emb_image_jina,\n",
    "    query_ids=query_ids,\n",
    "    doc_ids=doc_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG@10': 0.51109, 'MAP@10': 0.46715, 'R@10': 0.65128, 'P@10': 0.06513}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = eval_retrieval.evaluate_retrieval(qrels=qrels, results=run, k_values=[10])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG@10': 0.38448, 'MAP@10': 0.33982, 'R@10': 0.52718, 'P@10': 0.05272}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_jina = eval_retrieval.evaluate_retrieval(qrels=qrels, results=run_jina, k_values=[10])\n",
    "scores_jina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We find that although the Jina model is trained with search in mind, on the moderately descriptive captions of Flickr30k, MetaCLIP delivers a substantially stronger ability to perform text-to-image search (65% recall at 10 vs. 53%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNfhWdQ/cit74rXSzi60EDv",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02a10934bbb240a68c3f3d8eb2692743": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "180df961fd914402ad55bed72135a0dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b49458efdad466f8a09a89e3cffbf6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_180df961fd914402ad55bed72135a0dd",
      "placeholder": "​",
      "style": "IPY_MODEL_3b5c000ef2614e65bda8c8d6c8399a4b",
      "value": " 31014/31014 [06:43&lt;00:00, 92.89image/s]"
     }
    },
    "3b5c000ef2614e65bda8c8d6c8399a4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f0cf36a633e48acb50c164ab2bec5a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51b778274d4b42e9b192b54ea32aca23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "612ee24c34454687818bbb7dce1cb74a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cbcc61ecd5c47f69e1da6929863bdcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "881c722cbdf349e0904e94f33daa470e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51b778274d4b42e9b192b54ea32aca23",
      "max": 31014,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02a10934bbb240a68c3f3d8eb2692743",
      "value": 31014
     }
    },
    "9e12fd99f3fb4cbdad887affd3483b7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e2a60feed9354a1eafecfe9f6abd98bf",
       "IPY_MODEL_881c722cbdf349e0904e94f33daa470e",
       "IPY_MODEL_1b49458efdad466f8a09a89e3cffbf6b"
      ],
      "layout": "IPY_MODEL_4f0cf36a633e48acb50c164ab2bec5a1"
     }
    },
    "e2a60feed9354a1eafecfe9f6abd98bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_612ee24c34454687818bbb7dce1cb74a",
      "placeholder": "​",
      "style": "IPY_MODEL_7cbcc61ecd5c47f69e1da6929863bdcb",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
